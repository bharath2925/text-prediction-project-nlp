}
if(j == num_r) {
r_k <- SGT_DT$r[j]
} else {
r_k <- SGT_DT$r[j+1]
}
SGT_DT$Z[j] <- 2 * SGT_DT$n[j] / (r_k - r_i)
}
SGT_DT$logr <- log(SGT_DT$r)
SGT_DT$logZ <- log(SGT_DT$Z)
linearFit <- lm(SGT_DT$logZ ~ SGT_DT$logr)
c0 <- linearFit$coefficients[1]
c1 <- linearFit$coefficients[2]
use_y = FALSE
for (j in 1:(num_r-1)) {
r_plus_1 <- SGT_DT$r[j] + 1
s_r_plus_1 <- exp(c0 + (c1 * SGT_DT$logr[j+1]))
s_r <- exp(c0 + (c1 * SGT_DT$logr[j]))
y <- r_plus_1 * s_r_plus_1/s_r
if(use_y) {
SGT_DT$r_star[j] <- y
} else {
n_r_plus_1 <- SGT_DT$n[SGT_DT$r == r_plus_1]
if(length(n_r_plus_1) == 0 ) {
SGT_DT$r_star[j] <- y
use_y = TRUE
} else {
n_r <- SGT_DT$n[j]
x<-(r_plus_1) * n_r_plus_1/n_r
if (abs(x-y) > 1.96 * sqrt(((r_plus_1)^2) * (n_r_plus_1/((n_r)^2))*(1+(n_r_plus_1/n_r)))) {
SGT_DT$r_star[j] <- x
} else {
SGT_DT$r_star[j] <- y
use_y = TRUE
}
}
}
if(j==(num_r-1)) {
SGT_DT$r_star[j+1] <- y
}
}
N <- sum(SGT_DT$n * SGT_DT$r)
Nhat <- sum(SGT_DT$n * SGT_DT$r_star)
Po <- SGT_DT$n[1] / N
SGT_DT$p <- (1-Po) * SGT_DT$r_star/Nhat
return(SGT_DT)
}
SimpleGoodTuring <- list()
SimpleGoodTuring$w1w2w3 <- calculateSimpleGoodTuring(FourGram_Models$w1w2w3)
SimpleGoodTuring$w2w3 <- calculateSimpleGoodTuring(FourGram_Models$w2w3)
SimpleGoodTuring$w3 <- calculateSimpleGoodTuring(FourGram_Models$w3)
SimpleGoodTuring$w1w3 <- calculateSimpleGoodTuring(FourGram_Models$w1w3)
SimpleGoodTuring$w1w2 <- calculateSimpleGoodTuring(FourGram_Models$w1w2)
SimpleGoodTuring$w1 <- calculateSimpleGoodTuring(FourGram_Models$w1)
SimpleGoodTuring$w2 <- calculateSimpleGoodTuring(FourGram_Models$w2)
SimpleGoodTuring$w4 <- calculateSimpleGoodTuring(FourGram_Models$w4)
saveRDS(object = SimpleGoodTuring, file = "data/clean/sgt_model.rds")
FourGramPredict <- function(dframe, SimpleGoodTuring, finalList, ngram) {
if(nrow(ngram) > 0 & !(nrow(dframe) > 0)) {
ngram$Probability <- sapply(ngram$freq, FUN = function(x) SimpleGoodTuring$p[SimpleGoodTuring$r == x])
ngram <- ngram %>% select(w4, Probability)
if(!is.null(finalList) & nrow(ngram) > 0) {
ngram <- ngram %>% filter(w4 %in% finalList)
}
eval.parent(substitute(dframe <- ngram))
}
}
NextWordPredict <- function(sentence, FourGram_Model, SimpleGoodTuring, finalList=NULL) {
#options("scipen"=100, "digits"=8)
SentenceCount <- unlist(strsplit(sentence," "))
WordCount <- length(SentenceCount)
dframe <- data.frame(w4 = factor(), Probability = numeric())
FourGramPredict(dframe, SimpleGoodTuring$w1w2w3, finalList,
FourGram_Models$w1w2w3 %>% filter(w1 == SentenceCount[WordCount-2],
w2 == SentenceCount[WordCount-1],
w3 == SentenceCount[WordCount]))
FourGramPredict(dframe, SimpleGoodTuring$w2w3, finalList,
FourGram_Models$w2w3 %>% filter(w2 == SentenceCount[WordCount-1],
w3 == SentenceCount[WordCount]))
FourGramPredict(dframe, SimpleGoodTuring$w3, finalList,
FourGram_Models$w3 %>% filter(w3 == SentenceCount[WordCount]))
FourGramPredict(dframe, SimpleGoodTuring$w1w2, finalList,
FourGram_Models$w1w2 %>% filter(w1 == SentenceCount[WordCount-2],
w2 == SentenceCount[WordCount-1]))
FourGramPredict(dframe, SimpleGoodTuring$w1w3, finalList,
FourGram_Models$w1w3 %>% filter(w1 == SentenceCount[WordCount-2],
w3 == SentenceCount[WordCount]))
FourGramPredict(dframe, SimpleGoodTuring$w1, finalList,
FourGram_Models$w1 %>% filter(w1 == SentenceCount[WordCount-2]))
return(dframe %>% arrange(desc(Probability)))
}
WordPredict <- function(sentence) {
sentence <- stripWhitespace(sentence)
sentence <- tolower(sentence)
sentence <- removeNumbers(sentence)
sentence <- removePunctuation(sentence, preserve_intra_word_dashes = TRUE)
SentenceCount <- unlist(strsplit(sentence," "))
WordCount <- length(SentenceCount)
if(WordCount >= 3) {
return(NextWordPredict(paste(
SentenceCount[WordCount-2],
SentenceCount[WordCount-1],
SentenceCount[WordCount]), predictor.FourGram_Model, predictor.SimpleGoodTuring))
} else if(WordCount == 2) {
return(NextWordPredict(paste(
"-",
SentenceCount[WordCount-1],
SentenceCount[WordCount]), predictor.FourGram_Model, predictor.SimpleGoodTuring))
} else if(WordCount == 1) {
return(NextWordPredict(paste(
"-",
"-",
SentenceCount[WordCount]), predictor.FourGram_Model, predictor.SimpleGoodTuring))
}
}
predictor <- list()
predictor.FourGram_Model <- FourGram_Models
predictor.SimpleGoodTuring <- SimpleGoodTuring
predictor.WordPredict <- WordPredict
head(predictor.WordPredict("how are"), 15)
head(predictor.WordPredict("where are"), 15)
head(predictor.WordPredict("where is"), 15)
head(predictor.WordPredict("Can you"), 15)
head(predictor.WordPredict("Can you "), 15)
head(predictor.WordPredict("how are"), 15)
head(predictor.WordPredict("would you"), 15)
head(predictor.WordPredict("would you"), 5)
head(predictor.WordPredict("there are"), 5)
head(predictor.WordPredict("there were"), 5)
head(predictor.WordPredict("Bhara"), 5)
head(predictor.WordPredict("Bharath is "), 5)
source('~/datasciencecoursera/10Data Science Capstone Project/Course Project/Shiny/WordPrediction/model.R')
runApp('Shiny/WordPrediction')
getwd()
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
l<-predictor.WordPredict("where are")
head(l)
l %>%
ggplot(aes(x = w4, y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Frequent words occurred in the corpus sample", x = "1-gram")+
theme_minimal()
library(ggplot2)
l %>%
ggplot(aes(x = w4, y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Frequent words occurred in the corpus sample", x = "1-gram")+
theme_minimal()
l %>%
ggplot(aes(x = w4, y = Probability)) +
geom_bar() +
coord_flip() +
labs(title = "Frequent words occurred in the corpus sample", x = "1-gram")+
theme_minimal()
l %>%
ggplot(aes(x = w4, y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Frequent words occurred in the corpus sample", x = "1-gram")+
theme_minimal()
l %>%
ggplot(aes(x = w4, y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Top 10 Word Predictions", x = "Probabilities")+
theme_minimal()
l %>%
ggplot(aes(x = w4, y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Top 10 Word Predictions", x = "Probabilities", y = "Words")+
theme_minimal()
l %>%
ggplot(aes(x = w4, y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Top 10 Word Predictions", y = "Probabilities", x = "Words")+
theme_minimal()
k<-"where are"
l<-predictor.WordPredict("where are")
l<-predictor.WordPredict(k)
l %>%
ggplot(aes(x = w4, y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Top 10 Word Predictions", y = "Probabilities", x = "Words")+
theme_minimal()
l %>%
ggplot(aes(x = paste(k, " ", w4), y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Top 10 Word Predictions", y = "Probabilities", x = "Words")+
theme_minimal()
l %>%
ggplot(aes(x = paste(k, w4), y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Top 10 Word Predictions", y = "Probabilities", x = "Words")+
theme_minimal()
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp()
runApp('Shiny/WordPrediction')
l
order(l$Probability)
order(!l$Probability)
l[order(l$Probability)]
l[order(l$Probability),]
l[!order(l$Probability),]
l[order(-l$Probability),]
runApp('Shiny/WordPrediction')
l %>%
ggplot(aes(y = paste(k, w4), x = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Top 10 Word Predictions", y = "Probabilities", x = "Words")+
theme_minimal()
l %>%
ggplot(aes(y = paste(k, w4), x = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Top 10 Word Predictions", y = "Probabilities", x = "Words")+
theme_minimal()
l %>%
ggplot(aes(x = paste(k, w4), y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Top 10 Word Predictions", y = "Probabilities", x = "Words")+
theme_minimal()
l<-l[order(-l$Probability),]
l %>%
ggplot(aes(x = paste(k, w4), y = Probability)) +
geom_point() +
coord_flip() +
labs(title = "Top 10 Word Predictions", y = "Probabilities", x = "Words")+
theme_minimal()
l
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
nrow(l)
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
?req
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
shiny::runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
req(l[11,])
dtls
req(dtls[11,])
req(dtls[1,])
runApp('Shiny/WordPrediction')
shiny::runApp('Shiny/WordPrediction')
dtls
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp()
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
shiny::runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
temps<-predictor.predictWord("How are")
temps
predictor.predictWord("How are")
predictor.predictWord("how are")
predictor.predictWord("where are")
length(temps)
temps
runApp('Shiny/WordPrediction')
predictor.predictWord("ier")
temps<-predictor.predictWord("ier")
length(temps)
nrow(temps)
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
runApp('Shiny/WordPrediction')
gc()
#===========================1. Download Data=========================================
set.seed(03282021)
# Obtaining the data from the WEB
if(!dir.exists("data")) {
dir.create("data")
}
if(!file.exists("data/Coursera-SwiftKey.zip")) {
download.file(
url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",
destfile = "data/Coursera-SwiftKey.zip")
}
# Unziping the data in the current folder
if(file.exists("data/Coursera-SwiftKey.zip") &&
!dir.exists("data/raw/final")) {
unzip(zipfile = "data/Coursera-SwiftKey.zip",
exdir = "data/raw")
}
sampled_data <- c()
tweet<-readLines("data/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", warn = FALSE)
sampled_data<-c(sampled_data, sample(tweet, as.integer(length(tweet)*0.075)))
rm("tweet")
blogs<-readLines("data/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", warn = FALSE)
sampled_data<-c(sampled_data, sample(blogs, as.integer(length(blogs)*0.075)))
rm("blogs")
news<-readLines("data/Coursera-SwiftKey/final/en_US/en_US.news.txt", warn = FALSE)
sampled_data<-c(sampled_data, sample(news, as.integer(length(news)*0.075)))
rm("news")
sampled_data <- c()
tweet<-readLines("data/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", warn = FALSE)
sampled_data<-c(sampled_data, sample(tweet, as.integer(length(tweet)*0.0075)))
rm("tweet")
blogs<-readLines("data/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", warn = FALSE)
sampled_data<-c(sampled_data, sample(blogs, as.integer(length(blogs)*0.0075)))
rm("blogs")
news<-readLines("data/Coursera-SwiftKey/final/en_US/en_US.news.txt", warn = FALSE)
sampled_data<-c(sampled_data, sample(news, as.integer(length(news)*0.0075)))
rm("news")
clean_text<-function(text){
text<-replace_html(text)
text<-replace_internet_slang(text)
text<-replace_ordinal(text)
text<-replace_date(text)
text<-replace_money(text)
text<-replace_kern(text)
text<-replace_hash(text)
text<-replace_names(text)
text<-replace_incomplete(text, ".")
text<-replace_non_ascii(text)
text<-replace_number(text)
text<-gsub("[\\!-]+", "!", text)
text<-gsub("[\\?-]+", "?", text)
text<-gsub("(.)\\1{2,}", "\\1", text)
text<-gsub("fuck|shit|piss off|dickhead|asshole|bitch|bastard|damn|cunt|bugger|motherfucker", "", text, perl = TRUE)
}
#=========Sentences Cleaning Function==================
clean_sentences<-function(text){
#Tag all end of sentence punctuation to a string to be split later
#sentence<-data.frame(sentence = textshape::split_sentence(text))
sentence<-data.frame(sentence = unlist(tokenize_sentence(text)))
#Remove records which are blank or single whitespace
sentence<-subset(sentence, sentence!="")
sentence<-subset(sentence, sentence!=" ")
sentence$sentence<-gsub("(^| )b( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )c( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )d( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )e( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )f( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )g( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )h( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )z( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )j( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )k( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )l( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )m( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )n( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )o( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )p( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )q( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )r( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )s( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )t( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )u( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )v( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )w( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )x( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )y( |$)", "", sentence$sentence)
sentence$sentence<-gsub("NA", "", sentence$sentence)
#sentence<-subset(sentence, nchar(sentence)>0)
sentence
}
#=============Spell Check Function=========================
spell_check<-function(sentence){
bad<-hunspell(sentence)
len<-length(bad[[1]])
if(len >0){
for(i in 1:len){
sentence<-gsub(bad[[1]][i],
unlist(hunspell_suggest(bad[[1]][[i]]))[1],
sentence)
}
}
sentence<-gsub("[[:punct:]]", "", sentence)
sentence
}
#===========================================================
sampled_data<-clean_text(sampled_data)
library(tm)
sampled_data<-clean_text(sampled_data)
library(textclean)
gc()
#===========================1. Download Data=========================================
set.seed(03282021)
# Obtaining the data from the WEB
if(!dir.exists("data")) {
dir.create("data")
}
if(!file.exists("data/Coursera-SwiftKey.zip")) {
download.file(
url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",
destfile = "data/Coursera-SwiftKey.zip")
}
# Unziping the data in the current folder
if(file.exists("data/Coursera-SwiftKey.zip") &&
!dir.exists("data/raw/final")) {
unzip(zipfile = "data/Coursera-SwiftKey.zip",
exdir = "data/raw")
}
#==========================2. Sample the data=========================================================================
sampled_data <- c()
tweet<-readLines("data/Coursera-SwiftKey/final/en_US/en_US.twitter.txt", warn = FALSE)
sampled_data<-c(sampled_data, sample(tweet, as.integer(length(tweet)*0.0075)))
rm("tweet")
blogs<-readLines("data/Coursera-SwiftKey/final/en_US/en_US.blogs.txt", warn = FALSE)
sampled_data<-c(sampled_data, sample(blogs, as.integer(length(blogs)*0.0075)))
rm("blogs")
news<-readLines("data/Coursera-SwiftKey/final/en_US/en_US.news.txt", warn = FALSE)
sampled_data<-c(sampled_data, sample(news, as.integer(length(news)*0.0075)))
rm("news")
#===============Text Cleaning Function=================
clean_text<-function(text){
text<-replace_html(text)
text<-replace_internet_slang(text)
text<-replace_ordinal(text)
text<-replace_date(text)
text<-replace_money(text)
text<-replace_kern(text)
text<-replace_hash(text)
text<-replace_names(text)
text<-replace_incomplete(text, ".")
text<-replace_non_ascii(text)
text<-replace_number(text)
text<-gsub("[\\!-]+", "!", text)
text<-gsub("[\\?-]+", "?", text)
text<-gsub("(.)\\1{2,}", "\\1", text)
text<-gsub("fuck|shit|piss off|dickhead|asshole|bitch|bastard|damn|cunt|bugger|motherfucker", "", text, perl = TRUE)
}
#=========Sentences Cleaning Function==================
clean_sentences<-function(text){
#Tag all end of sentence punctuation to a string to be split later
#sentence<-data.frame(sentence = textshape::split_sentence(text))
sentence<-data.frame(sentence = unlist(tokenize_sentence(text)))
#Remove records which are blank or single whitespace
sentence<-subset(sentence, sentence!="")
sentence<-subset(sentence, sentence!=" ")
sentence$sentence<-gsub("(^| )b( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )c( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )d( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )e( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )f( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )g( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )h( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )z( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )j( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )k( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )l( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )m( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )n( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )o( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )p( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )q( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )r( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )s( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )t( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )u( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )v( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )w( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )x( |$)", "", sentence$sentence)
sentence$sentence<-gsub("(^| )y( |$)", "", sentence$sentence)
sentence$sentence<-gsub("NA", "", sentence$sentence)
#sentence<-subset(sentence, nchar(sentence)>0)
sentence
}
#=============Spell Check Function=========================
spell_check<-function(sentence){
bad<-hunspell(sentence)
len<-length(bad[[1]])
if(len >0){
for(i in 1:len){
sentence<-gsub(bad[[1]][i],
unlist(hunspell_suggest(bad[[1]][[i]]))[1],
sentence)
}
}
sentence<-gsub("[[:punct:]]", "", sentence)
sentence
}
#===========================================================
sampled_data<-clean_text(sampled_data)
sampled_data<-clean_sentences(sampled_data)
library(dplyr)
library(quanteda)
sampled_data<-clean_sentences(sampled_data)
sampled_data$sentence<-sapply(sampled_data$sentence, spell_check)
library(hunspell)
sampled_data$sentence<-sapply(sampled_data$sentence, spell_check)
